{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c331f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300f7b2",
   "metadata": {},
   "source": [
    "parametri/config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18c4e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "\n",
    "RANDOM_STATE = 1219\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.1\n",
    "WORKERS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d8d3a",
   "metadata": {},
   "source": [
    "definisanje fja za premestanje na gpu, ako je dostupan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6d77ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "  return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def bind_gpu(data):\n",
    "  device = get_device()\n",
    "  if isinstance(data, (list, tuple)):\n",
    "    return [bind_gpu(data_elem) for data_elem in data]\n",
    "  else:\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ecfcd9",
   "metadata": {},
   "source": [
    "dataset klasa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatexDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for image-to-LaTeX project.\n",
    "    Each item is (image_tensor, formula_string)\n",
    "    \"\"\"\n",
    "    def __init__(self, data_type: str, transform=None):\n",
    "        super().__init__()\n",
    "        assert data_type in ['train', 'test', 'validate', 'testtest'], 'Not found data type'\n",
    "        self.transform = transform\n",
    "\n",
    "        csv_path = os.path.join(data_path, f'im2latex_{data_type}.csv')\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # promenimo kolonu image tako da ima ceo put do fajla\n",
    "        df['image'] = df.image.map(lambda x: os.path.join(os.path.join(\"data\", \"formula_images_processed\"), f'{x}'))\n",
    "        # TODO: pojasni si sta je tacno walker\n",
    "        self.walker = df.to_dict('records')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.walker)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.walker[idx]\n",
    "        \n",
    "        formula = item['formula']\n",
    "        image = torchvision.io.read_image(str(item['image']))\n",
    "        \n",
    "        return image, formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc4ebbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75275, 8370, 10355)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_set = LatexDataset('train')\n",
    "val_set = LatexDataset('validate')\n",
    "test_set = LatexDataset('test')\n",
    "test_test = LatexDataset('testtest')\n",
    "\n",
    "len(train_set), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd0e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\zjaks\\AppData\\Local\\Temp\\ipykernel_21132\\2670044509.py:14: SyntaxWarning: invalid escape sequence '\\['\n",
      "  \"(\\\\\\\\[a-zA-Z]+)|\" + '((\\\\\\\\)*[$-/:-?{-~!\"^_`\\[\\]])|' + \"(\\w)|\" + \"(\\\\\\\\)\"\n",
      "C:\\Users\\zjaks\\AppData\\Local\\Temp\\ipykernel_21132\\2670044509.py:14: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  \"(\\\\\\\\[a-zA-Z]+)|\" + '((\\\\\\\\)*[$-/:-?{-~!\"^_`\\[\\]])|' + \"(\\w)|\" + \"(\\\\\\\\)\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from torch import Tensor\n",
    "\n",
    "class Text():\n",
    "    def __init__(self):\n",
    "        self.pad_id = 0\n",
    "        self.sos_id = 1\n",
    "        self.eos_id = 2\n",
    "        \n",
    "        self.id2word = json.load(open(\"data/vocab/100k_vocab.json\", \"r\"))\n",
    "        self.word2id = dict(zip(self.id2word, range(len(self.id2word))))\n",
    "        self.TOKENIZE_PATTERN = re.compile(\n",
    "            r\"(\\\\[a-zA-Z]+)|\"           # LaTeX commands like \\frac, \\sqrt\n",
    "            r\"((\\\\)*[$-/:-?{-~!\\\"^_`\\[\\]])|\"  # math symbols\n",
    "            r\"(\\w)|\"                    # single letters/numbers\n",
    "            r\"(\\\\)\"                     # stray backslashes\n",
    "            )\n",
    "        self.n_class = len(self.id2word)\n",
    "\n",
    "    def int2text(self, x: Tensor):\n",
    "        return \" \".join([self.id2word[i] for i in x if i > self.eos_id])\n",
    "\n",
    "    def text2int(self, formula: str):\n",
    "        return torch.LongTensor([self.word2id[i] for i in self.tokenize(formula)])\n",
    "\n",
    "    def tokenize(self, formula: str):\n",
    "        tokens = re.finditer(self.TOKENIZE_PATTERN, formula)\n",
    "        tokens = list(map(lambda x: x.group(0), tokens))\n",
    "        tokens = [x for x in tokens if x is not None and x != \"\"]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b24edc7",
   "metadata": {},
   "source": [
    "data module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24d74f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence\n",
    "\n",
    "text = Text()\n",
    "\n",
    "def collate_fn(batch, text):\n",
    "    formulas = [text.text2int(i[1]) for i in batch]\n",
    "    formulas = pad_sequence(formulas, batch_first=True)\n",
    "    sos = torch.zeros(BATCH_SIZE, 1) + text.sos_id\n",
    "    eos = torch.zeros(BATCH_SIZE, 1) + text.eos_id\n",
    "    formulas = torch.cat((sos, formulas, eos), dim=-1).to(dtype=torch.long)\n",
    "    image = [i[0] for i in batch]\n",
    "    max_width, max_height = 0, 0\n",
    "    for img in image:\n",
    "        c, h, w = img.size()\n",
    "        max_width = max(max_width, w)\n",
    "        max_height = max(max_height, h)\n",
    "    pad = torchvision.transforms.Resize(size=(max_height, max_width))\n",
    "    image = torch.stack(list(map(lambda x: pad(x), image))).to(dtype=torch.float)\n",
    "    return image, formulas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56daed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_test_loader = DataLoader(test_test, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, collate_fn=lambda batch: collate_fn(batch, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b445eb7",
   "metadata": {},
   "source": [
    "primer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eaab48c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 3, 160, 480])\n",
      "torch.Size([1024, 193])\n"
     ]
    }
   ],
   "source": [
    "for images, formulas in test_test_loader:\n",
    "    print(images.shape)      # [32, 3, H, W]\n",
    "    print(formulas.shape)    # [32, max_formula_len+2]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9de70a",
   "metadata": {},
   "source": [
    "### Enkoder\n",
    "* input je slika s 3 kanala (RGB)\n",
    "* output feature map ima `enc_dim` kanala\n",
    "* `nn.MaxPool2d(2, 1)`  -> asimetricni pooling: praktikuje se za slike teksta, jer je tekst vise sirok nego visok\n",
    "\n",
    "##### Forward pass\n",
    "input tenzor za forward pass: `x: (bs, c, w, h)`\n",
    "\n",
    "`bc = batch size`\n",
    "\n",
    "`c = number of channels`\n",
    "\n",
    "`w = image width`\n",
    "\n",
    "`h = image height`\n",
    "\n",
    "1. enkodovati `x(bc, c_in, w_in, h_in) -> (bc, c_out, w_out, h_out) `\n",
    "    *  `c_out` je `enc_dim`\n",
    "    * `w_out` i `h_out` manji od dimenzija inputa (ofc, conv mreza)\n",
    "2. permutovati -> da bi feature vector bio poslednji\n",
    "3. flattenovati\n",
    "* `encoder_out.shape = (bs, sequence_length = w_out * h_out, d = enc_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010181d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mConvEncoder\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, enc_dim: \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from torch import nn, Tensor\n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, enc_dim: int):\n",
    "        super().__init__()\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1),\n",
    "            nn.Conv2d(64, 128, 3, 1),\n",
    "            nn.Conv2d(128, 256, 3, 1),\n",
    "            nn.Conv2d(256, 256, 3, 1),\n",
    "            nn.MaxPool2d(2, 1),\n",
    "            nn.Conv2d(256, 512, 3, 1),\n",
    "            nn.MaxPool2d(1, 2),\n",
    "            nn.Conv2d(512, enc_dim, 3, 1),\n",
    "        )\n",
    "        self.enc_dim = enc_dim\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "            x: (bs, c, w, h)\n",
    "        \"\"\"\n",
    "        encoder_out = self.feature_encoder(x)  # (bs, c, w, h)\n",
    "        encoder_out = encoder_out.permute(0, 2, 3, 1)  # (bs, w, h, c)\n",
    "        bs, _, _, d = encoder_out.size()\n",
    "        encoder_out = encoder_out.view(bs, -1, d)\n",
    "        return encoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b4a584-8b42-4d7e-ba92-e030f1d34d6f",
   "metadata": {},
   "source": [
    "# Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494e9d00-104c-43ae-b330-b05ca5411998",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAttention\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, enoder_dim: \u001b[38;5;28mint\u001b[39m = \u001b[32m512\u001b[39m, decoder_dim: \u001b[38;5;28mint\u001b[39m = \u001b[32m512\u001b[39m, attention_dim: \u001b[38;5;28mint\u001b[39m = \u001b[32m512\u001b[39m):\n\u001b[32m      3\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enoder_dim: int = 512, decoder_dim: int = 512, attention_dim: int = 512):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Racunamo kontekst vektor na osnovu sledecih jednacina\n",
    "        e = tanh((Wₕhₜ₋₁ + bₕ) + (WᵥV + bᵥ))  \n",
    "        αₜ = Softmax(Wₐ·e + bₐ)  \n",
    "        cₜ = ∑ᵢ αₜⁱ vᵢ, where vᵢ ∈ V  \n",
    "        \"\"\"\n",
    "        self.decoder_attention = nn.Linear(decoder_dim, attention_dim, bias=False) # W_h * h_{t-1}\n",
    "        self.encoder_attention = nn.Linear(enoder_dim, attention_dim, bias=False) # W_V * V\n",
    "        self.attention = nn.Linear(attention_dim, 1, bias=False)      # W_a * attn\n",
    "       \n",
    "        # Softmax će pretvoriti sirove rezultate u raspodelu verovatnoće (težine pažnje).\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, h: Tensor, V: Tensor):\n",
    "        \"\"\"\n",
    "        Izračunaj kontekst vektor tako što pažljivo posmatraš najrelevantnije delove slike.\n",
    "       \n",
    "        Argumenti:\n",
    "            h: Prethodno skriveno stanje LSTM dekodera. Oblik: (batch_size, decoder_dim)\n",
    "            V: Mapa karakteristika. Oblik: (batch_size, w * h, encoder_dim)\n",
    "               \n",
    "        Povratna vrednost:\n",
    "            context (Tensor): Vektor koji iz mapa karakteristike izvlaci relevantne podatke za generisanje sledeceg karaktera.\n",
    "                            Oblik: (batch_size, decoder_dime)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        attn_1 = self.decoder_attention(h) #(b, decoder_dim) -> (b, attention_dim)\n",
    "        attn_2 = self.encoder_attention(V) #(b, w*h, enoder_dim) -> (b, w*h, attention_dim)\n",
    "       \n",
    "        attention= self.attention(torch.tanh(attn_1.unsqueeze(1) + attn_2)).squeeze(2)\n",
    "        # attn_1.unsqueeze(1): (b, 1, attention_dim)\n",
    "        # attn_2: (b, w*h, attention_dim)\n",
    "        # tanh(): (b, w*h, attention_dim)\n",
    "        # attention: (b, w*h, 1) -> squeeze(2) -> (b, w*h)\n",
    "       \n",
    "        alpha = self.softmax(attention)\n",
    "       \n",
    "       \n",
    "        context = (alpha.unsqueeze(2) * V).sum(dim=1)\n",
    "        # alpha.unsqueeze(2): (b, w*h, 1)\n",
    "        # V: (b, w*h, enoder_dim)\n",
    "        # product: (b, w*h, enoder_dim)\n",
    "        # context: (b, enoder_dim)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58861f50-9f77-4a8d-9099-c410a079e676",
   "metadata": {},
   "source": [
    "# Dekoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bed4949-ab58-4b38-b5ce-0262ad890cf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDecoder\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,n_class: \u001b[38;5;28mint\u001b[39m,embedding_dim: \u001b[38;5;28mint\u001b[39m = \u001b[32m80\u001b[39m,encoder_dim: \u001b[38;5;28mint\u001b[39m = \u001b[32m512\u001b[39m,decoder_dim: \u001b[38;5;28mint\u001b[39m = \u001b[32m512\u001b[39m,attention_dim: \u001b[38;5;28mint\u001b[39m = \u001b[32m512\u001b[39m,\n\u001b[32m      3\u001b[39m         num_layers: \u001b[38;5;28mint\u001b[39m = \u001b[32m1\u001b[39m,dropout: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.1\u001b[39m,bidirectional: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,sos_id: \u001b[38;5;28mint\u001b[39m = \u001b[32m1\u001b[39m,eos_id: \u001b[38;5;28mint\u001b[39m = \u001b[32m2\u001b[39m):\n\u001b[32m      4\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,n_class: int,embedding_dim: int = 80,encoder_dim: int = 512,decoder_dim: int = 512,attention_dim: int = 512,\n",
    "        num_layers: int = 1,dropout: float = 0.1,bidirectional: bool = False,sos_id: int = 1,eos_id: int = 2):\n",
    "        super().__init__()\n",
    "       \n",
    "        \"\"\"\n",
    "        Implementacija dekodera za Image-to-Latex model.\n",
    "        Koristi LSTM ćeliju i Luong pažnju da generiše LaTeX simbole korak po korak.\n",
    "        cₜ = Attention(hₜ₋₁, V)\n",
    "        eₜ = Embedding(yₜ)\n",
    "        (oₜ, hₜ) = LSTM(hₜ₋₁, [cₜ, eₜ])\n",
    "        p(yₜ₊₁ | y₁, ..., yₜ) = Softmax(Wₒ · oₜ + bₒ)\n",
    "       \n",
    "        \"\"\"\n",
    "\n",
    "        self.sos_id = sos_id\n",
    "        self.eos_id = eos_id\n",
    "       \n",
    "        # Embedding layer konvertuje token ID u vektor\n",
    "        self.embedding = nn.Embedding(n_class, embedding_dim)  # (vocab_size, embedding_dim)\n",
    "       \n",
    "        # Instanca mehanizma pažnje\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # Veličina enkodera -> veličina pažnje\n",
    "       \n",
    "        # Linearni sloj za spajanje embeddinga i konteksta pažnje\n",
    "        self.concat = nn.Linear(embedding_dim + encoder_dim, decoder_dim)  # (embedding_dim + encoder_dim) -> decoder_dim\n",
    "       \n",
    "        # Prvi LSTM sloj\n",
    "        self.rnn = nn.LSTM(\n",
    "            decoder_dim,\n",
    "            decoder_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "       \n",
    "        # Dropout za regularizaciju\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "       \n",
    "        # Drugi LSTM sloj za dublji model\n",
    "        self.rnn2 = nn.LSTM(\n",
    "            decoder_dim,\n",
    "            decoder_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "       \n",
    "        # Izlazni sloj koji preslikuje u prostor rečnika\n",
    "        self.out = nn.Linear(decoder_dim, n_class)  # (decoder_dim) -> (n_class)\n",
    "       \n",
    "        # LogSoftmax za stabilnost prilikom računanja gubitka\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        # Inicijalizacija težina\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        if isinstance(layer, nn.Embedding):\n",
    "            nn.init.orthogonal_(layer.weight)\n",
    "        elif isinstance(layer, nn.LSTM):\n",
    "            for name, param in self.rnn.named_parameters():\n",
    "                if name.startswith(\"weight\"):\n",
    "                    nn.init.orthogonal_(param)\n",
    "\n",
    "    def forward(self, y, encoder_out=None, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Generiše sledeći token na osnovu trenutnog stanja i izlaza enkodera.\n",
    "       \n",
    "        Argumenti:\n",
    "            y: Ulazni tokeni. Oblik: (batch_size, target_len)\n",
    "            encoder_out: Izlaz enkodera (V). Oblik: (batch_size, encoder_dim, w', h')\n",
    "            hidden_state: Prethodno skriveno stanje (h, c). Oblik: (num_layers * num_directions, batch_size, decoder_dim)\n",
    "               \n",
    "        Povratna vrednost:\n",
    "            out: Log-verovatnoće za sledeći token. Oblik: (batch_size, 1, n_class)\n",
    "            hidden_state: Ažurirano skriveno stanje.\n",
    "        \"\"\"\n",
    "\n",
    "        h, c = hidden_state  # (b, decoder_dim), (b, decoder_dim)\n",
    "       \n",
    "        embed = self.embedding(y)  # (b, seq_len, embedding_dim)\n",
    "        attention_context = self.attention(h, encoder_out)  # (b, encoder_dim)\n",
    "       \n",
    "        rnn_input = torch.cat([embed[:, -1], attention_context], dim=1)  # (b, embedding_dim + encoder_dim)\n",
    "        rnn_input = self.concat(rnn_input)  # (b, decoder_dim)\n",
    "       \n",
    "        rnn_input = rnn_input.unsqueeze(1)  # (b, 1, decoder_dim)\n",
    "        hidden_state = (h.unsqueeze(0), c.unsqueeze(0))  # (1, b, decoder_dim), (1, b, decoder_dim)\n",
    "       \n",
    "        out, hidden_state = self.rnn(rnn_input, hidden_state)  # out: (b, 1, decoder_dim)\n",
    "       \n",
    "        out = self.dropout(out)  # (b, 1, decoder_dim)\n",
    "       \n",
    "        out, hidden_state = self.rnn2(out, hidden_state)  # out: (b, 1, decoder_dim)\n",
    "        out = self.logsoftmax(self.out(out))  # (b, 1, n_class)\n",
    "       \n",
    "        h, c = hidden_state\n",
    "        return out, (h.squeeze(0), c.squeeze(0))  # Squeeze dimenziju slojeva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4997b2-f22c-44b0-9fff-97bb1c6439d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
