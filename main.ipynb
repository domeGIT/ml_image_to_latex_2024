{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c331f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300f7b2",
   "metadata": {},
   "source": [
    "parametri/config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c4e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "\n",
    "RANDOM_STATE = 1219\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.1\n",
    "WORKERS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d8d3a",
   "metadata": {},
   "source": [
    "definisanje fja za premestanje na gpu, ako je dostupan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d77ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "  return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def bind_gpu(data):\n",
    "  device = get_device()\n",
    "  if isinstance(data, (list, tuple)):\n",
    "    return [bind_gpu(data_elem) for data_elem in data]\n",
    "  else:\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ecfcd9",
   "metadata": {},
   "source": [
    "dataset klasa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5faa108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatexDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for image-to-LaTeX project.\n",
    "    Each item is (image_tensor, formula_string)\n",
    "    \"\"\"\n",
    "    def __init__(self, data_type: str, transform=None):\n",
    "        super().__init__()\n",
    "        assert data_type in ['train', 'test', 'validate', 'testtest'], 'Not found data type'\n",
    "        self.transform = transform\n",
    "\n",
    "        csv_path = os.path.join(data_path, f'im2latex_{data_type}.csv')\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # promenimo kolonu image tako da ima ceo put do fajla\n",
    "        df['image'] = df.image.map(lambda x: os.path.join(os.path.join(\"data\", \"formula_images_processed\"), f'{x}'))\n",
    "        # TODO: pojasni si sta je tacno walker\n",
    "        self.walker = df.to_dict('records')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.walker)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.walker[idx]\n",
    "        \n",
    "        formula = item['formula']\n",
    "        image = torchvision.io.read_image(str(item['image']))\n",
    "        \n",
    "        return image, formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc4ebbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75275, 8370, 10355)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_set = LatexDataset('train')\n",
    "val_set = LatexDataset('validate')\n",
    "test_set = LatexDataset('test')\n",
    "test_test = LatexDataset('testtest')\n",
    "\n",
    "len(train_set), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61cd0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from torch import Tensor\n",
    "\n",
    "class Text():\n",
    "    def __init__(self):\n",
    "        self.pad_id = 0\n",
    "        self.sos_id = 1\n",
    "        self.eos_id = 2\n",
    "        \n",
    "        self.id2word = json.load(open(\"data/vocab/100k_vocab.json\", \"r\"))\n",
    "        self.word2id = dict(zip(self.id2word, range(len(self.id2word))))\n",
    "        self.TOKENIZE_PATTERN = re.compile(\n",
    "            r\"(\\\\[a-zA-Z]+)|\"           # LaTeX commands like \\frac, \\sqrt\n",
    "            r\"((\\\\)*[$-/:-?{-~!\\\"^_`\\[\\]])|\"  # math symbols\n",
    "            r\"(\\w)|\"                    # single letters/numbers\n",
    "            r\"(\\\\)\"                     # stray backslashes\n",
    "            )\n",
    "        self.n_class = len(self.id2word)\n",
    "\n",
    "    def int2text(self, x: Tensor):\n",
    "        return \" \".join([self.id2word[i] for i in x if i > self.eos_id])\n",
    "\n",
    "    def text2int(self, formula: str):\n",
    "        return torch.LongTensor([self.word2id[i] for i in self.tokenize(formula)])\n",
    "\n",
    "    def tokenize(self, formula: str):\n",
    "        tokens = re.finditer(self.TOKENIZE_PATTERN, formula)\n",
    "        tokens = list(map(lambda x: x.group(0), tokens))\n",
    "        tokens = [x for x in tokens if x is not None and x != \"\"]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b24edc7",
   "metadata": {},
   "source": [
    "data module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d74f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence\n",
    "\n",
    "text = Text()\n",
    "\n",
    "def collate_fn(batch, text):\n",
    "    formulas = [text.text2int(i[1]) for i in batch]\n",
    "    formulas = pad_sequence(formulas, batch_first=True)\n",
    "    sos = torch.zeros(BATCH_SIZE, 1) + text.sos_id\n",
    "    eos = torch.zeros(BATCH_SIZE, 1) + text.eos_id\n",
    "    formulas = torch.cat((sos, formulas, eos), dim=-1).to(dtype=torch.long)\n",
    "    image = [i[0] for i in batch]\n",
    "    max_width, max_height = 0, 0\n",
    "    for img in image:\n",
    "        c, h, w = img.size()\n",
    "        max_width = max(max_width, w)\n",
    "        max_height = max(max_height, h)\n",
    "    pad = torchvision.transforms.Resize(size=(max_height, max_width))\n",
    "    image = torch.stack(list(map(lambda x: pad(x), image))).to(dtype=torch.float)\n",
    "    return image, formulas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56daed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_test_loader = DataLoader(test_test, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, collate_fn=lambda batch: collate_fn(batch, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b445eb7",
   "metadata": {},
   "source": [
    "primer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab48c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 128, 480])\n",
      "torch.Size([16, 187])\n"
     ]
    }
   ],
   "source": [
    "for images, formulas in test_test_loader:\n",
    "    print(images.shape)      # [batch_size, 3, H, W]\n",
    "    print(formulas.shape)    # [batch_size, max_formula_len+2]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9de70a",
   "metadata": {},
   "source": [
    "### Enkoder\n",
    "* input je slika s 3 kanala (RGB)\n",
    "* output feature map ima `enc_dim` kanala\n",
    "* `nn.MaxPool2d(2, 1)`  -> asimetricni pooling: praktikuje se za slike teksta, jer je tekst vise sirok nego visok\n",
    "\n",
    "##### Forward pass\n",
    "input tenzor za forward pass: `x: (bs, c, w, h)`\n",
    "\n",
    "`bc = batch size`\n",
    "\n",
    "`c = number of channels`\n",
    "\n",
    "`w = image width`\n",
    "\n",
    "`h = image height`\n",
    "\n",
    "1. enkodovati `x(bc, c_in, w_in, h_in) -> (bc, c_out, w_out, h_out) `\n",
    "    *  `c_out` je `enc_dim`\n",
    "    * `w_out` i `h_out` manji od dimenzija inputa (ofc, conv mreza)\n",
    "2. permutovati -> da bi feature vector bio poslednji\n",
    "3. flattenovati\n",
    "* `encoder_out.shape = (bs, sequence_length = w_out * h_out, d = enc_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "010181d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, encoder_dim: int):\n",
    "        super().__init__()\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1),\n",
    "            nn.Conv2d(64, 128, 3, 1),\n",
    "            nn.Conv2d(128, 256, 3, 1),\n",
    "            nn.Conv2d(256, 256, 3, 1),\n",
    "            nn.MaxPool2d(2, 1),\n",
    "            nn.Conv2d(256, 512, 3, 1),\n",
    "            nn.MaxPool2d(1, 2),\n",
    "            nn.Conv2d(512, encoder_dim, 3, 1),\n",
    "        )\n",
    "        self.encoder_dim = encoder_dim\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "            x: (bs, c, w, h)\n",
    "            encoder_out: (batch_size, seq_len = width*height, feat_dim = d)\n",
    "        \"\"\"\n",
    "        encoder_out = self.feature_encoder(x)  # (bs, c, w, h) ali c i w manji\n",
    "        encoder_out = encoder_out.permute(0, 2, 3, 1)  # (bs, w, h, c) poslednja dim je feature depth d\n",
    "        bs, _, _, d = encoder_out.size()\n",
    "        encoder_out = encoder_out.view(bs, -1, d) # izravnaj (w, h) matricu u niz w*h tako da (bs, w, h, c) -> (bs, w*h, c)\n",
    "        return encoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b4a584-8b42-4d7e-ba92-e030f1d34d6f",
   "metadata": {},
   "source": [
    "# Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "494e9d00-104c-43ae-b330-b05ca5411998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enoder_dim: int = 512, decoder_dim: int = 512, attention_dim: int = 512):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Racunamo kontekst vektor na osnovu sledecih jednacina\n",
    "        e = tanh((Wₕhₜ₋₁ + bₕ) + (WᵥV + bᵥ))  \n",
    "        αₜ = Softmax(Wₐ·e + bₐ)  \n",
    "        cₜ = ∑ᵢ αₜⁱ vᵢ, where vᵢ ∈ V  \n",
    "        \"\"\"\n",
    "        self.decoder_attention = nn.Linear(decoder_dim, attention_dim, bias=False) # W_h * h_{t-1}\n",
    "        self.encoder_attention = nn.Linear(enoder_dim, attention_dim, bias=False) # W_V * V\n",
    "        self.attention = nn.Linear(attention_dim, 1, bias=False)      # W_a * attn\n",
    "       \n",
    "        # Softmax će pretvoriti sirove rezultate u raspodelu verovatnoće (težine pažnje).\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, h: Tensor, V: Tensor):\n",
    "        \"\"\"\n",
    "        Izračunaj kontekst vektor tako što pažljivo posmatraš najrelevantnije delove slike.\n",
    "       \n",
    "        Argumenti:\n",
    "            h: Prethodno skriveno stanje LSTM dekodera. Oblik: (batch_size, decoder_dim)\n",
    "            V: Mapa karakteristika. Oblik: (batch_size, w * h, encoder_dim)\n",
    "               \n",
    "        Povratna vrednost:\n",
    "            context (Tensor): Vektor koji iz mapa karakteristike izvlaci relevantne podatke za generisanje sledeceg karaktera.\n",
    "                            Oblik: (batch_size, decoder_dime)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        attn_1 = self.decoder_attention(h) #(b, decoder_dim) -> (b, attention_dim)\n",
    "        attn_2 = self.encoder_attention(V) #(b, w*h, enoder_dim) -> (b, w*h, attention_dim)\n",
    "       \n",
    "        attention= self.attention(torch.tanh(attn_1.unsqueeze(1) + attn_2)).squeeze(2)\n",
    "        # attn_1.unsqueeze(1): (b, 1, attention_dim)\n",
    "        # attn_2: (b, w*h, attention_dim)\n",
    "        # tanh(): (b, w*h, attention_dim)\n",
    "        # attention: (b, w*h, 1) -> squeeze(2) -> (b, w*h)\n",
    "       \n",
    "        alpha = self.softmax(attention)\n",
    "       \n",
    "       \n",
    "        context = (alpha.unsqueeze(2) * V).sum(dim=1)\n",
    "        # alpha.unsqueeze(2): (b, w*h, 1)\n",
    "        # V: (b, w*h, enoder_dim)\n",
    "        # product: (b, w*h, enoder_dim)\n",
    "        # context: (b, enoder_dim)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58861f50-9f77-4a8d-9099-c410a079e676",
   "metadata": {},
   "source": [
    "# Dekoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bed4949-ab58-4b38-b5ce-0262ad890cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,n_class: int,embedding_dim: int = 80,encoder_dim: int = 512,decoder_dim: int = 512,attention_dim: int = 512,\n",
    "        num_layers: int = 1,dropout: float = 0.1,bidirectional: bool = False,sos_id: int = 1,eos_id: int = 2):\n",
    "        super().__init__()\n",
    "       \n",
    "        \"\"\"\n",
    "        Implementacija dekodera za Image-to-Latex model.\n",
    "        Koristi LSTM ćeliju i Luong pažnju da generiše LaTeX simbole korak po korak.\n",
    "        cₜ = Attention(hₜ₋₁, V)\n",
    "        eₜ = Embedding(yₜ)\n",
    "        (oₜ, hₜ) = LSTM(hₜ₋₁, [cₜ, eₜ])\n",
    "        p(yₜ₊₁ | y₁, ..., yₜ) = Softmax(Wₒ · oₜ + bₒ)\n",
    "       \n",
    "        \"\"\"\n",
    "\n",
    "        self.sos_id = sos_id\n",
    "        self.eos_id = eos_id\n",
    "       \n",
    "        # Embedding layer konvertuje token ID u vektor\n",
    "        self.embedding = nn.Embedding(n_class, embedding_dim)  # (vocab_size, embedding_dim)\n",
    "       \n",
    "        # Instanca mehanizma pažnje\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # Veličina enkodera -> veličina pažnje\n",
    "       \n",
    "        # Linearni sloj za spajanje embeddinga i konteksta pažnje\n",
    "        self.concat = nn.Linear(embedding_dim + encoder_dim, decoder_dim)  # (embedding_dim + encoder_dim) -> decoder_dim\n",
    "       \n",
    "        # Prvi LSTM sloj\n",
    "        self.rnn = nn.LSTM(\n",
    "            decoder_dim,\n",
    "            decoder_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "       \n",
    "        # Dropout za regularizaciju\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "       \n",
    "        # Drugi LSTM sloj za dublji model\n",
    "        self.rnn2 = nn.LSTM(\n",
    "            decoder_dim,\n",
    "            decoder_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "       \n",
    "        # Izlazni sloj koji preslikuje u prostor rečnika\n",
    "        self.out = nn.Linear(decoder_dim, n_class)  # (decoder_dim) -> (n_class)\n",
    "       \n",
    "        # LogSoftmax za stabilnost prilikom računanja gubitka\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        # Inicijalizacija težina\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        if isinstance(layer, nn.Embedding):\n",
    "            nn.init.orthogonal_(layer.weight)\n",
    "        elif isinstance(layer, nn.LSTM):\n",
    "            for name, param in self.rnn.named_parameters():\n",
    "                if name.startswith(\"weight\"):\n",
    "                    nn.init.orthogonal_(param)\n",
    "\n",
    "    def forward(self, y, encoder_out=None, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Generiše sledeći token na osnovu trenutnog stanja i izlaza enkodera.\n",
    "       \n",
    "        Argumenti:\n",
    "            y: Ulazni tokeni. Oblik: (batch_size, target_len)\n",
    "            encoder_out: Izlaz enkodera (V). Oblik: (batch_size, encoder_dim, w', h')\n",
    "            hidden_state: Prethodno skriveno stanje (h, c). Oblik: (num_layers * num_directions, batch_size, decoder_dim)\n",
    "               \n",
    "        Povratna vrednost:\n",
    "            out: Log-verovatnoće za sledeći token. Oblik: (batch_size, 1, n_class)\n",
    "            hidden_state: Ažurirano skriveno stanje.\n",
    "        \"\"\"\n",
    "\n",
    "        h, c = hidden_state  # (b, decoder_dim), (b, decoder_dim)\n",
    "       \n",
    "        embed = self.embedding(y)  # (b, seq_len, embedding_dim)\n",
    "        attention_context = self.attention(h, encoder_out)  # (b, encoder_dim)\n",
    "       \n",
    "        rnn_input = torch.cat([embed[:, -1], attention_context], dim=1)  # (b, embedding_dim + encoder_dim)\n",
    "        rnn_input = self.concat(rnn_input)  # (b, decoder_dim)\n",
    "       \n",
    "        rnn_input = rnn_input.unsqueeze(1)  # (b, 1, decoder_dim)\n",
    "        hidden_state = (h.unsqueeze(0), c.unsqueeze(0))  # (1, b, decoder_dim), (1, b, decoder_dim)\n",
    "       \n",
    "        out, hidden_state = self.rnn(rnn_input, hidden_state)  # out: (b, 1, decoder_dim)\n",
    "       \n",
    "        out = self.dropout(out)  # (b, 1, decoder_dim)\n",
    "       \n",
    "        out, hidden_state = self.rnn2(out, hidden_state)  # out: (b, 1, decoder_dim)\n",
    "        out = self.logsoftmax(self.out(out))  # (b, 1, n_class)\n",
    "       \n",
    "        h, c = hidden_state\n",
    "        return out, (h.squeeze(0), c.squeeze(0))  # Squeeze dimenziju slojeva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f4997b2-f22c-44b0-9fff-97bb1c6439d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image2LatexModel(nn.Module):\n",
    "    def __init__(self,n_class: int,embedding_dim: int = 80,encoder_dim: int = 512,decoder_dim: int = 512,attention_dim: int = 512,\n",
    "        num_layers: int = 1,dropout: float = 0.1,bidirectional: bool = False,text: Text = None, beam_width: int = 5, sos_id: int = 1,eos_id: int = 2):\n",
    "        super().__init__()\n",
    "        self.encoder = ConvEncoder(encoder_dim=encoder_dim)\n",
    "        self.decoder = Decoder(n_class=n_class,embedding_dim=embedding_dim,encoder_dim=encoder_dim,decoder_dim=decoder_dim,attention_dim=attention_dim,num_layers=num_layers,dropout=dropout,bidirectional=bidirectional,sos_id=sos_id,eos_id=eos_id)\n",
    "\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.n_class = n_class\n",
    "        self.text = text\n",
    "        self.beam_width = beam_width\n",
    "        self.encoder = ConvEncoder(encoder_dim=encoder_dim)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # TODO: why do we init the hidden state like this and not just 0?\n",
    "    def init_decoder_hidden_state(self, V: Tensor):\n",
    "        \"\"\"\n",
    "            input V je autput enkodera (bs, w*h, c)\n",
    "            return (h, c)\n",
    "        \"\"\"\n",
    "        encoder_mean = V.mean(dim=1)\n",
    "        h = torch.tanh(self.init_h(encoder_mean))\n",
    "        c = torch.tanh(self.init_c(encoder_mean))\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, x: Tensor, y: Tensor, y_len: Tensor):\n",
    "        encoder_out = self.encoder(x)\n",
    "\n",
    "        hidden_state = self.init_decoder_hidden_state(encoder_out)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for t in range(y_len.max().item()):\n",
    "            dec_input = y[:, t].unsqueeze(1)\n",
    "            out, hidden_state = self.decoder(dec_input, encoder_out, hidden_state)\n",
    "            predictions.append(out.squeeze(1))\n",
    "\n",
    "        predictions = torch.stack(predictions, dim=1)\n",
    "        return predictions\n",
    "    \n",
    "    def decode_beam_search(self, x, max_length=150):\n",
    "        encoder_out = self.encoder(x)\n",
    "        hidden_state = self.init_decoder_hidden_state(encoder_out)\n",
    "\n",
    "        list_candidate = [([self.decoder.sos_id], hidden_state, 0)]\n",
    "        for t in range(max_length):\n",
    "            new_candidates = []\n",
    "            for inp, state, log_prob in list_candidate:\n",
    "                y = torch.LongTensor([inp[-1]]).view(BATCH_SIZE, -1).to(x.device)\n",
    "                out, hidden_state = self.decoder(y, encoder_out, state)\n",
    "\n",
    "                topk = out.topk(self.beam_width)\n",
    "                for val, idx in zip(topk.values.view(-1), topk.indices.view(-1)):\n",
    "                    new_inp = inp + [idx.item()]\n",
    "                    new_candidates.append((new_inp, hidden_state, log_prob + val.item()))\n",
    "\n",
    "            new_candidates = sorted(new_candidates, key=lambda x: x[2], reverse=True)\n",
    "            list_candidate = new_candidates[: self.beam_width]\n",
    "\n",
    "        return list_candidate[0][0]\n",
    "    \n",
    "    def decode(self, x, max_length=150):\n",
    "        predict = self.decode_beam_search(x, max_length)\n",
    "        return self.text.int2text(predict)\n",
    "    \n",
    "    def compute_loss(self, outputs, formulas_out):\n",
    "        \"\"\"\n",
    "        Compute CrossEntropy loss between predictions and targets.\n",
    "        outputs: (batch, seq_len, vocab_size) logits\n",
    "        formulas_out: (batch, seq_len) target token IDs\n",
    "        \"\"\"\n",
    "        bs, t, _ = outputs.size()\n",
    "        return self.criterion(\n",
    "            outputs.reshape(bs * t, -1),   # flatten predictions\n",
    "            formulas_out.reshape(-1)       # flatten targets\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7e57c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image2LatexModel(\n",
      "  (encoder): ConvEncoder(\n",
      "    (feature_encoder): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (4): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (6): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(100, 80)\n",
      "    (attention): Attention(\n",
      "      (decoder_attention): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (encoder_attention): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (attention): Linear(in_features=512, out_features=1, bias=False)\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (concat): Linear(in_features=592, out_features=512, bias=True)\n",
      "    (rnn): LSTM(512, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (rnn2): LSTM(512, 512, batch_first=True)\n",
      "    (out): Linear(in_features=512, out_features=100, bias=True)\n",
      "    (logsoftmax): LogSoftmax(dim=-1)\n",
      "  )\n",
      "  (init_h): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (init_c): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test 1: hoce li se instancirati?\n",
    "model = Image2LatexModel(n_class=100)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3de188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 100])\n"
     ]
    }
   ],
   "source": [
    "# test 2 : forward pass random data\n",
    "\n",
    "channels = 1\n",
    "height = 64\n",
    "width = 256\n",
    "\n",
    "# Fake images\n",
    "batch_size = 1\n",
    "\n",
    "# Fake integer image input (values between 0 and 255 like pixel values)\n",
    "# x = torch.randint(0, 256, (batch_size, 3, 64, 256), dtype=torch.int64)\n",
    "x = torch.randn(batch_size, 3, 64, 256)\n",
    "\n",
    "# Target sequences (integer token IDs)\n",
    "y = torch.randint(0, 10, (batch_size, 50), dtype=torch.int64)\n",
    "# y = torch.randn(batch_size, 50)\n",
    "seq_len = y.size(1)\n",
    "y_len = torch.randint(1, seq_len + 1, (batch_size,))\n",
    "\n",
    "# Forward pass (you might also need target sequences if your forward expects them)\n",
    "out = model(x, y, y_len)\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
