{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c331f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300f7b2",
   "metadata": {},
   "source": [
    "parametri/config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c4e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "\n",
    "# kandidat za num of workers? (os.cpu_count())\n",
    "\n",
    "RANDOM_STATE = 1219\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.1\n",
    "WORKERS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ecfcd9",
   "metadata": {},
   "source": [
    "dataset klasa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5faa108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatexDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for image-to-LaTeX project.\n",
    "    Each item is (image_tensor, formula_string)\n",
    "    \"\"\"\n",
    "    def __init__(self, data_type: str, transform=None):\n",
    "        super().__init__()\n",
    "        assert data_type in ['train', 'test', 'validate', 'testtest'], 'Not found data type'\n",
    "        self.transform = transform\n",
    "\n",
    "        csv_path = os.path.join(data_path, f'im2latex_{data_type}.csv')\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # promenimo kolonu image tako da ima ceo put do fajla\n",
    "        df['image'] = df.image.map(lambda x: os.path.join(os.path.join(\"data\", \"formula_images_processed\"), f'{x}'))\n",
    "        # TODO: pojasni si sta je tacno walker\n",
    "        self.walker = df.to_dict('records')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.walker)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.walker[idx]\n",
    "        \n",
    "        formula = item['formula']\n",
    "        image = torchvision.io.read_image(str(item['image']))\n",
    "        \n",
    "        return image, formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc4ebbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75275, 8370, 10355)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_set = LatexDataset('train')\n",
    "val_set = LatexDataset('validate')\n",
    "test_set = LatexDataset('test')\n",
    "# test_test = LatexDataset('testtest')\n",
    "\n",
    "len(train_set), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61cd0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from torch import Tensor\n",
    "\n",
    "class Text():\n",
    "    def __init__(self):\n",
    "        self.pad_id = 0\n",
    "        self.sos_id = 1\n",
    "        self.eos_id = 2\n",
    "        \n",
    "        self.id2word = json.load(open(\"data/vocab/100k_vocab.json\", \"r\"))\n",
    "        self.word2id = dict(zip(self.id2word, range(len(self.id2word))))\n",
    "        self.TOKENIZE_PATTERN = re.compile(\n",
    "            r\"(\\\\[a-zA-Z]+)|\"           # LaTeX commands like \\frac, \\sqrt\n",
    "            r\"((\\\\)*[$-/:-?{-~!\\\"^_`\\[\\]])|\"  # math symbols\n",
    "            r\"(\\w)|\"                    # single letters/numbers\n",
    "            r\"(\\\\)\"                     # stray backslashes\n",
    "            )\n",
    "        self.n_class = len(self.id2word)\n",
    "\n",
    "    def int2text(self, x: Tensor):\n",
    "        return \" \".join([self.id2word[i] for i in x if i > self.eos_id])\n",
    "\n",
    "    def text2int(self, formula: str):\n",
    "        return torch.LongTensor([self.word2id[i] for i in self.tokenize(formula)])\n",
    "\n",
    "    def tokenize(self, formula: str):\n",
    "        tokens = re.finditer(self.TOKENIZE_PATTERN, formula)\n",
    "        tokens = list(map(lambda x: x.group(0), tokens))\n",
    "        tokens = [x for x in tokens if x is not None and x != \"\"]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b24edc7",
   "metadata": {},
   "source": [
    "data module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d74f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence\n",
    "\n",
    "# TODO: ako ostane vremena, DIY vocab builder\n",
    "\n",
    "text = Text()\n",
    "\n",
    "def collate_fn(batch, text):\n",
    "    # convert formulas to token IDs\n",
    "    formulas = [text.text2int(str(i[1])) for i in batch]   # ensure string\n",
    "    formula_len = torch.tensor([len(f) + 1 for f in formulas], dtype=torch.long)\n",
    "    formulas = pad_sequence(formulas, batch_first=True)\n",
    "\n",
    "    batch_size = len(batch)\n",
    "    sos = torch.full((batch_size, 1), text.sos_id, dtype=torch.long)\n",
    "    eos = torch.full((batch_size, 1), text.eos_id, dtype=torch.long)\n",
    "\n",
    "    formulas = torch.cat((sos, formulas, eos), dim=-1)\n",
    "\n",
    "    # pad images to same size\n",
    "    images = [i[0] for i in batch]\n",
    "    max_width, max_height = 0, 0\n",
    "    for img in images:\n",
    "        c, h, w = img.size()\n",
    "        max_width = max(max_width, w)\n",
    "        max_height = max(max_height, h)\n",
    "\n",
    "    pad = torchvision.transforms.Resize(size=(max_height, max_width))\n",
    "    images = torch.stack([pad(img) for img in images]).to(dtype=torch.float)\n",
    "\n",
    "    return images, formulas, formula_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56daed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, collate_fn=lambda batch: collate_fn(batch, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b445eb7",
   "metadata": {},
   "source": [
    "primer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaab48c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 64, 384])\n",
      "torch.Size([16, 112])\n"
     ]
    }
   ],
   "source": [
    "for images, formulas, _ in test_loader:\n",
    "    print(images.shape)      # [batch_size, 3, H, W]\n",
    "    print(formulas.shape)    # [batch_size, max_formula_len+2]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9de70a",
   "metadata": {},
   "source": [
    "### Enkoder\n",
    "* input je slika s 3 kanala (RGB)\n",
    "* output feature map ima `enc_dim` kanala\n",
    "* `nn.MaxPool2d(2, 1)`  -> asimetricni pooling: praktikuje se za slike teksta, jer je tekst vise sirok nego visok\n",
    "\n",
    "##### Forward pass\n",
    "input tenzor za forward pass: `x: (bs, c, w, h)`\n",
    "\n",
    "`bc = batch size`\n",
    "\n",
    "`c = number of channels`\n",
    "\n",
    "`w = image width`\n",
    "\n",
    "`h = image height`\n",
    "\n",
    "1. enkodovati `x(bc, c_in, w_in, h_in) -> (bc, c_out, w_out, h_out) `\n",
    "    *  `c_out` je `enc_dim`\n",
    "    * `w_out` i `h_out` manji od dimenzija inputa (ofc, conv mreza)\n",
    "2. permutovati -> da bi feature vector bio poslednji\n",
    "3. flattenovati\n",
    "* `encoder_out.shape = (bs, sequence_length = w_out * h_out, d = enc_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "010181d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, encoder_dim: int):\n",
    "        super().__init__()\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1),\n",
    "            nn.Conv2d(64, 128, 3, 1),\n",
    "            nn.Conv2d(128, 256, 3, 1),\n",
    "            nn.Conv2d(256, 256, 3, 1),\n",
    "            nn.MaxPool2d(2, 1),\n",
    "            nn.Conv2d(256, 512, 3, 1),\n",
    "            nn.MaxPool2d(1, 2),\n",
    "            nn.Conv2d(512, encoder_dim, 3, 1),\n",
    "        )\n",
    "        self.encoder_dim = encoder_dim\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "            x: (bs, c, w, h)\n",
    "            encoder_out: (batch_size, seq_len = width*height, feat_dim = d)\n",
    "        \"\"\"\n",
    "        encoder_out = self.feature_encoder(x)  # (bs, c, w, h) ali c i w manji\n",
    "        encoder_out = encoder_out.permute(0, 2, 3, 1)  # (bs, w, h, c) poslednja dim je feature depth d\n",
    "        bs, _, _, d = encoder_out.size()\n",
    "        encoder_out = encoder_out.view(bs, -1, d) # izravnaj (w, h) matricu u niz w*h tako da (bs, w, h, c) -> (bs, w*h, c)\n",
    "        return encoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b4a584-8b42-4d7e-ba92-e030f1d34d6f",
   "metadata": {},
   "source": [
    "# Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494e9d00-104c-43ae-b330-b05ca5411998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enoder_dim: int = 512, decoder_dim: int = 512, attention_dim: int = 512):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Racunamo kontekst vektor na osnovu sledecih jednacina\n",
    "        e = tanh((Wₕhₜ₋₁ + bₕ) + (WᵥV + bᵥ))  \n",
    "        αₜ = Softmax(Wₐ·e + bₐ)  \n",
    "        cₜ = ∑ᵢ αₜⁱ vᵢ, where vᵢ ∈ V  \n",
    "        \"\"\"\n",
    "        self.decoder_attention = nn.Linear(decoder_dim, attention_dim, bias=False) # W_h * h_{t-1}\n",
    "        self.encoder_attention = nn.Linear(enoder_dim, attention_dim, bias=False) # W_V * V\n",
    "        self.attention = nn.Linear(attention_dim, 1, bias=False)      # W_a * attn\n",
    "       \n",
    "        # Softmax će pretvoriti sirove rezultate u raspodelu verovatnoće (težine pažnje).\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, h: Tensor, V: Tensor):\n",
    "        \"\"\"\n",
    "        Izračunaj kontekst vektor tako što pažljivo posmatraš najrelevantnije delove slike.\n",
    "       \n",
    "        Argumenti:\n",
    "            h: Prethodno skriveno stanje LSTM dekodera. Oblik: (batch_size, decoder_dim)\n",
    "            V: Mapa karakteristika. Oblik: (batch_size, w * h, encoder_dim)\n",
    "               \n",
    "        Povratna vrednost:\n",
    "            context (Tensor): Vektor koji iz mapa karakteristike izvlaci relevantne podatke za generisanje sledeceg karaktera.\n",
    "                            Oblik: (batch_size, decoder_dime)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        attn_1 = self.decoder_attention(h) #(b, decoder_dim) -> (b, attention_dim)\n",
    "        attn_2 = self.encoder_attention(V) #(b, w*h, enoder_dim) -> (b, w*h, attention_dim)\n",
    "       \n",
    "        attention= self.attention(torch.tanh(attn_1.unsqueeze(1) + attn_2)).squeeze(2)\n",
    "        # attn_1.unsqueeze(1): (b, 1, attention_dim)\n",
    "        # attn_2: (b, w*h, attention_dim)\n",
    "        # tanh(): (b, w*h, attention_dim)\n",
    "        # attention: (b, w*h, 1) -> squeeze(2) -> (b, w*h)\n",
    "       \n",
    "        alpha = self.softmax(attention)\n",
    "       \n",
    "       \n",
    "        context = (alpha.unsqueeze(2) * V).sum(dim=1)\n",
    "        # alpha.unsqueeze(2): (b, w*h, 1)\n",
    "        # V: (b, w*h, enoder_dim)\n",
    "        # product: (b, w*h, enoder_dim)\n",
    "        # context: (b, enoder_dim)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58861f50-9f77-4a8d-9099-c410a079e676",
   "metadata": {},
   "source": [
    "# Dekoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bed4949-ab58-4b38-b5ce-0262ad890cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,n_class: int,embedding_dim: int = 80,encoder_dim: int = 512,decoder_dim: int = 512,attention_dim: int = 512,\n",
    "        num_layers: int = 1,dropout: float = 0.1,bidirectional: bool = False,sos_id: int = 1,eos_id: int = 2):\n",
    "        super().__init__()\n",
    "       \n",
    "        \"\"\"\n",
    "        Implementacija dekodera za Image-to-Latex model.\n",
    "        Koristi LSTM ćeliju i Luong pažnju da generiše LaTeX simbole korak po korak.\n",
    "        cₜ = Attention(hₜ₋₁, V)\n",
    "        eₜ = Embedding(yₜ)\n",
    "        (oₜ, hₜ) = LSTM(hₜ₋₁, [cₜ, eₜ])\n",
    "        p(yₜ₊₁ | y₁, ..., yₜ) = Softmax(Wₒ · oₜ + bₒ)\n",
    "       \n",
    "        \"\"\"\n",
    "\n",
    "        self.sos_id = sos_id\n",
    "        self.eos_id = eos_id\n",
    "       \n",
    "        # Embedding layer konvertuje token ID u vektor\n",
    "        self.embedding = nn.Embedding(n_class, embedding_dim)  # (vocab_size, embedding_dim)\n",
    "       \n",
    "        # Instanca mehanizma pažnje\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # Veličina enkodera -> veličina pažnje\n",
    "       \n",
    "        # Linearni sloj za spajanje embeddinga i konteksta pažnje\n",
    "        self.concat = nn.Linear(embedding_dim + encoder_dim, decoder_dim)  # (embedding_dim + encoder_dim) -> decoder_dim\n",
    "       \n",
    "        # Prvi LSTM sloj\n",
    "        self.rnn = nn.LSTM(\n",
    "            decoder_dim,\n",
    "            decoder_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "       \n",
    "        # Dropout za regularizaciju\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "       \n",
    "        # Drugi LSTM sloj za dublji model\n",
    "        self.rnn2 = nn.LSTM(\n",
    "            decoder_dim,\n",
    "            decoder_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "       \n",
    "        # Izlazni sloj koji preslikuje u prostor rečnika\n",
    "        self.out = nn.Linear(decoder_dim, n_class)  # (decoder_dim) -> (n_class)\n",
    "       \n",
    "        # LogSoftmax za stabilnost prilikom računanja gubitka\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        # Inicijalizacija težina\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        if isinstance(layer, nn.Embedding):\n",
    "            nn.init.orthogonal_(layer.weight)\n",
    "        elif isinstance(layer, nn.LSTM):\n",
    "            for name, param in self.rnn.named_parameters():\n",
    "                if name.startswith(\"weight\"):\n",
    "                    nn.init.orthogonal_(param)\n",
    "\n",
    "    def forward(self, y, encoder_out=None, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Generiše sledeći token na osnovu trenutnog stanja i izlaza enkodera.\n",
    "       \n",
    "        Argumenti:\n",
    "            y: Ulazni tokeni. Oblik: (batch_size, target_len)\n",
    "            encoder_out: Izlaz enkodera (V). Oblik: (batch_size, encoder_dim, w', h')\n",
    "            hidden_state: Prethodno skriveno stanje (h, c). Oblik: (num_layers * num_directions, batch_size, decoder_dim)\n",
    "               \n",
    "        Povratna vrednost:\n",
    "            out: Log-verovatnoće za sledeći token. Oblik: (batch_size, 1, n_class)\n",
    "            hidden_state: Ažurirano skriveno stanje.\n",
    "        \"\"\"\n",
    "\n",
    "        h, c = hidden_state  # (b, decoder_dim), (b, decoder_dim)\n",
    "       \n",
    "        embed = self.embedding(y)  # (b, seq_len, embedding_dim)\n",
    "        attention_context = self.attention(h, encoder_out)  # (b, encoder_dim)\n",
    "       \n",
    "        rnn_input = torch.cat([embed[:, -1], attention_context], dim=1)  # (b, embedding_dim + encoder_dim)\n",
    "        rnn_input = self.concat(rnn_input)  # (b, decoder_dim)\n",
    "       \n",
    "        rnn_input = rnn_input.unsqueeze(1)  # (b, 1, decoder_dim)\n",
    "        hidden_state = (h.unsqueeze(0), c.unsqueeze(0))  # (1, b, decoder_dim), (1, b, decoder_dim)\n",
    "       \n",
    "        out, hidden_state = self.rnn(rnn_input, hidden_state)  # out: (b, 1, decoder_dim)\n",
    "       \n",
    "        out = self.dropout(out)  # (b, 1, decoder_dim)\n",
    "       \n",
    "        out, hidden_state = self.rnn2(out, hidden_state)  # out: (b, 1, decoder_dim)\n",
    "        out = self.logsoftmax(self.out(out))  # (b, 1, n_class)\n",
    "       \n",
    "        h, c = hidden_state\n",
    "        return out, (h.squeeze(0), c.squeeze(0))  # Squeeze dimenziju slojeva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f4997b2-f22c-44b0-9fff-97bb1c6439d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image2LatexModel(nn.Module):\n",
    "    def __init__(self,n_class: int,embedding_dim: int = 80,encoder_dim: int = 512,decoder_dim: int = 512,attention_dim: int = 512,\n",
    "        num_layers: int = 1,dropout: float = 0.1,bidirectional: bool = False,text: Text = None, beam_width: int = 5, sos_id: int = 1,eos_id: int = 2):\n",
    "        super().__init__()\n",
    "        self.encoder = ConvEncoder(encoder_dim=encoder_dim)\n",
    "        self.decoder = Decoder(n_class=n_class,embedding_dim=embedding_dim,encoder_dim=encoder_dim,decoder_dim=decoder_dim,attention_dim=attention_dim,num_layers=num_layers,dropout=dropout,bidirectional=bidirectional,sos_id=sos_id,eos_id=eos_id)\n",
    "\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.n_class = n_class\n",
    "        self.text = text\n",
    "        self.beam_width = beam_width\n",
    "        self.encoder = ConvEncoder(encoder_dim=encoder_dim)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # TODO: why do we init the hidden state like this and not just 0?\n",
    "    def init_decoder_hidden_state(self, V: Tensor):\n",
    "        \"\"\"\n",
    "            input V je autput enkodera (bs, w*h, c)\n",
    "            return (h, c)\n",
    "        \"\"\"\n",
    "        encoder_mean = V.mean(dim=1)\n",
    "        h = torch.tanh(self.init_h(encoder_mean))\n",
    "        c = torch.tanh(self.init_c(encoder_mean))\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, x: Tensor, y: Tensor, y_len: Tensor):\n",
    "        encoder_out = self.encoder(x)\n",
    "\n",
    "        hidden_state = self.init_decoder_hidden_state(encoder_out)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for t in range(y_len.max().item()):\n",
    "            dec_input = y[:, t].unsqueeze(1)\n",
    "            out, hidden_state = self.decoder(dec_input, encoder_out, hidden_state)\n",
    "            predictions.append(out.squeeze(1))\n",
    "\n",
    "        predictions = torch.stack(predictions, dim=1)\n",
    "        return predictions\n",
    "    \n",
    "    def decode_beam_search(self, x, max_length=150):\n",
    "        encoder_out = self.encoder(x)\n",
    "        hidden_state = self.init_decoder_hidden_state(encoder_out)\n",
    "\n",
    "        list_candidate = [([self.decoder.sos_id], hidden_state, 0)]\n",
    "        for t in range(max_length):\n",
    "            new_candidates = []\n",
    "            for inp, state, log_prob in list_candidate:\n",
    "                y = torch.LongTensor([inp[-1]]).view(BATCH_SIZE, -1).to(x.device)\n",
    "                out, hidden_state = self.decoder(y, encoder_out, state)\n",
    "\n",
    "                topk = out.topk(self.beam_width)\n",
    "                for val, idx in zip(topk.values.view(-1), topk.indices.view(-1)):\n",
    "                    new_inp = inp + [idx.item()]\n",
    "                    new_candidates.append((new_inp, hidden_state, log_prob + val.item()))\n",
    "\n",
    "            new_candidates = sorted(new_candidates, key=lambda x: x[2], reverse=True)\n",
    "            list_candidate = new_candidates[: self.beam_width]\n",
    "\n",
    "        return list_candidate[0][0]\n",
    "    \n",
    "    def decode(self, x, max_length=150):\n",
    "        predict = self.decode_beam_search(x, max_length)\n",
    "        return self.text.int2text(predict)\n",
    "    \n",
    "    def compute_loss(self, outputs, formulas_out):\n",
    "        \"\"\"\n",
    "        Compute CrossEntropy loss between predictions and targets.\n",
    "        outputs: (batch, seq_len, vocab_size) logits\n",
    "        formulas_out: (batch, seq_len) target token IDs\n",
    "        \"\"\"\n",
    "        bs, t, _ = outputs.size()\n",
    "        return self.criterion(\n",
    "            outputs.reshape(bs * t, -1),   # flatten predictions\n",
    "            formulas_out.reshape(-1)       # flatten targets\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7e57c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image2LatexModel(\n",
      "  (encoder): ConvEncoder(\n",
      "    (feature_encoder): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (4): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (6): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(100, 80)\n",
      "    (attention): Attention(\n",
      "      (decoder_attention): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (encoder_attention): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (attention): Linear(in_features=512, out_features=1, bias=False)\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (concat): Linear(in_features=592, out_features=512, bias=True)\n",
      "    (rnn): LSTM(512, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (rnn2): LSTM(512, 512, batch_first=True)\n",
      "    (out): Linear(in_features=512, out_features=100, bias=True)\n",
      "    (logsoftmax): LogSoftmax(dim=-1)\n",
      "  )\n",
      "  (init_h): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (init_c): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test 1: hoce li se instancirati?\n",
    "\n",
    "model = Image2LatexModel(n_class=100)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3de188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 100])\n"
     ]
    }
   ],
   "source": [
    "# test 2 : forward pass random data\n",
    "\n",
    "channels = 1\n",
    "height = 64\n",
    "width = 256\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Fake image input (values floats)\n",
    "x = torch.randn(batch_size, 3, 64, 256)\n",
    "\n",
    "# Target sequences (fake integer token IDs)\n",
    "y = torch.randint(0, 10, (batch_size, 50), dtype=torch.int64)\n",
    "\n",
    "seq_len = y.size(1)\n",
    "y_len = torch.randint(1, seq_len + 1, (batch_size,))\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, y, y_len)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cba7ae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     69\u001b[39m formulas_in = formulas[:, :-\u001b[32m1\u001b[39m]\n\u001b[32m     70\u001b[39m formulas_out = formulas[:, \u001b[32m1\u001b[39m:]\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformulas_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformula_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m loss = criterion(outputs.reshape(-\u001b[32m1\u001b[39m, outputs.shape[-\u001b[32m1\u001b[39m]), formulas_out.reshape(-\u001b[32m1\u001b[39m))\n\u001b[32m     74\u001b[39m train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjaksic\\PERSONAL\\ml_image_to_latex_2024\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjaksic\\PERSONAL\\ml_image_to_latex_2024\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mImage2LatexModel.forward\u001b[39m\u001b[34m(self, x, y, y_len)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(y_len.max().item()):\n\u001b[32m     35\u001b[39m     dec_input = y[:, t].unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     out, hidden_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     predictions.append(out.squeeze(\u001b[32m1\u001b[39m))\n\u001b[32m     39\u001b[39m predictions = torch.stack(predictions, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjaksic\\PERSONAL\\ml_image_to_latex_2024\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjaksic\\PERSONAL\\ml_image_to_latex_2024\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mDecoder.forward\u001b[39m\u001b[34m(self, y, encoder_out, hidden_state)\u001b[39m\n\u001b[32m     80\u001b[39m h, c = hidden_state  \u001b[38;5;66;03m# (b, decoder_dim), (b, decoder_dim)\u001b[39;00m\n\u001b[32m     82\u001b[39m embed = \u001b[38;5;28mself\u001b[39m.embedding(y)  \u001b[38;5;66;03m# (b, seq_len, embedding_dim)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m attention_context = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (b, encoder_dim)\u001b[39;00m\n\u001b[32m     85\u001b[39m rnn_input = torch.cat([embed[:, -\u001b[32m1\u001b[39m], attention_context], dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (b, embedding_dim + encoder_dim)\u001b[39;00m\n\u001b[32m     86\u001b[39m rnn_input = \u001b[38;5;28mself\u001b[39m.concat(rnn_input)  \u001b[38;5;66;03m# (b, decoder_dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjaksic\\PERSONAL\\ml_image_to_latex_2024\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjaksic\\PERSONAL\\ml_image_to_latex_2024\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, h, V)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# attn_1.unsqueeze(1): (b, 1, attention_dim)\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# attn_2: (b, w*h, attention_dim)\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# tanh(): (b, w*h, attention_dim)\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# attention: (b, w*h, 1) -> squeeze(2) -> (b, w*h)\u001b[39;00m\n\u001b[32m     41\u001b[39m alpha = \u001b[38;5;28mself\u001b[39m.softmax(attention)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m context = \u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# alpha.unsqueeze(2): (b, w*h, 1)\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# V: (b, w*h, enoder_dim)\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# product: (b, w*h, enoder_dim)\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# context: (b, enoder_dim)\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m context\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "MAX_LENGTH=150\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def bind_gpu(data):\n",
    "    device = get_device()\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [bind_gpu(data_elem) for data_elem in data]\n",
    "    else:\n",
    "        return data.to(device, non_blocking=True)\n",
    "\n",
    "# Priprema podataka\n",
    "text_processor = Text()\n",
    "\n",
    "#plz ucitaj ih\n",
    "train_dataset = LatexDataset('train')\n",
    "val_dataset = LatexDataset('validate')\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # important for training\n",
    "    num_workers=WORKERS,\n",
    "    collate_fn=lambda batch: collate_fn(batch, text)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # usually keep validation deterministic\n",
    "    num_workers=WORKERS,\n",
    "    collate_fn=lambda batch: collate_fn(batch, text)\n",
    ")\n",
    "\n",
    "# Inicijalizacija modela,greske,scheduler,optimizera\n",
    "device = get_device() \n",
    "\n",
    "model = Image2LatexModel(\n",
    "    n_class=text_processor.n_class,\n",
    "    text=text_processor,\n",
    "    beam_width=5,\n",
    "    sos_id=text_processor.sos_id,\n",
    "    eos_id=text_processor.eos_id\n",
    ").to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.002, betas=(0.9, 0.98))\n",
    "\n",
    "total_steps = len(train_loader) * N_EPOCHS\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.002, total_steps=total_steps)\n",
    "\n",
    "#treninng\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "       \n",
    "        images, formulas, formula_len = bind_gpu(batch)\n",
    "\n",
    "        formulas_in = formulas[:, :-1]\n",
    "        formulas_out = formulas[:, 1:]\n",
    "\n",
    "        outputs = model(images, formulas_in, formula_len)\n",
    "        loss = criterion(outputs.reshape(-1, outputs.shape[-1]), formulas_out.reshape(-1))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Average Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "  \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_bleu = 0.0\n",
    "    val_em = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "          \n",
    "            images, formulas, formula_len = bind_gpu(batch)\n",
    "            formulas_in = formulas[:, :-1]\n",
    "            formulas_out = formulas[:, 1:]\n",
    "\n",
    "            outputs = model(images, formulas_in, formula_len)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[-1]), formulas_out.reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predicts = []\n",
    "            truths = []\n",
    "            for img, formula in zip(images, formulas):\n",
    "                pred_tokens = model.decode(img.unsqueeze(0), max_length=MAX_LENGTH)\n",
    "                truth_tokens = formula.tolist()\n",
    "               \n",
    "                predicts.append(pred_tokens)\n",
    "                truths.append(truth_tokens)\n",
    "           \n",
    "            predict_strings = [text_processor.tokenize(text_processor.int2text(p)) for p in predicts]\n",
    "            truth_strings = [text_processor.tokenize(text_processor.int2text(t)) for t in truths]\n",
    "           \n",
    "            bleu4 = calculate_bleu_score(predict_strings, truth_strings)\n",
    "            em = calculate_exact_match(predict_strings, truth_strings)\n",
    "\n",
    "            val_bleu += bleu4\n",
    "            val_em += em\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_bleu = val_bleu / len(val_loader)\n",
    "    avg_val_em = val_em / len(val_loader)\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, BLEU4: {avg_val_bleu:.4f}, EM: {avg_val_em:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "torch.save(model.state_dict(), 'image2latex_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
