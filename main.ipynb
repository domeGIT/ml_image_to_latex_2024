{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5ce199-37e2-48b4-ab73-15181fea2db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300f7b2",
   "metadata": {},
   "source": [
    "parametri/config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c4e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: vratiti na staro, promenjeno zbog google colaba\n",
    "data_path = \"data\"\n",
    "\n",
    "RANDOM_STATE = 1219\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.1\n",
    "WORKERS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ecfcd9",
   "metadata": {},
   "source": [
    "dataset klasa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5faa108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class LatexDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for image-to-LaTeX project.\n",
    "    Each item is (image_tensor, formula_string)\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path: str, transform=None):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # promenimo kolonu image tako da ima ceo put do fajla\n",
    "        df['image'] = df.image.map(lambda x: os.path.join('/content/data/formula_images_processed', f'{x}'))\n",
    "        # TODO: pojasni si sta je tacno walker\n",
    "        self.walker = df.to_dict('records')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.walker)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.walker[idx]\n",
    "\n",
    "        formula = item['formula']\n",
    "        image = torchvision.io.read_image(str(item['image']))\n",
    "        image = TF.rgb_to_grayscale(image, num_output_channels=1)  # (1, H, W)\n",
    "\n",
    "        return image, formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61cd0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from torch import Tensor\n",
    "\n",
    "class Text():\n",
    "    def __init__(self):\n",
    "        self.pad_id = 0\n",
    "        self.sos_id = 1\n",
    "        self.eos_id = 2\n",
    "        \n",
    "        self.id2word = json.load(open(\"data/vocab/100k_vocab.json\", \"r\"))\n",
    "        self.word2id = dict(zip(self.id2word, range(len(self.id2word))))\n",
    "        self.TOKENIZE_PATTERN = re.compile(\n",
    "            r\"(\\\\[a-zA-Z]+)|\"           # LaTeX commands like \\frac, \\sqrt\n",
    "            r\"((\\\\)*[$-/:-?{-~!\\\"^_`\\[\\]])|\"  # math symbols\n",
    "            r\"(\\w)|\"                    # single letters/numbers\n",
    "            r\"(\\\\)\"                     # stray backslashes\n",
    "            )\n",
    "        self.n_class = len(self.id2word)\n",
    "\n",
    "    def int2text(self, x: Tensor):\n",
    "        return \" \".join([self.id2word[i] for i in x if i > self.eos_id])\n",
    "\n",
    "    def text2int(self, formula: str):\n",
    "        return torch.LongTensor([self.word2id[i] for i in self.tokenize(formula)])\n",
    "\n",
    "    def tokenize(self, formula: str):\n",
    "        tokens = re.finditer(self.TOKENIZE_PATTERN, formula)\n",
    "        tokens = list(map(lambda x: x.group(0), tokens))\n",
    "        tokens = [x for x in tokens if x is not None and x != \"\"]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b24edc7",
   "metadata": {},
   "source": [
    "collate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85977e7d-ef7a-453c-a43b-bcd4d492905c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "rom torchvision import transforms\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "text = Text()\n",
    "\n",
    "def collate_fn(batch, text):\n",
    "\n",
    "    formulas = [text.text2int(str(i[1])) for i in batch]\n",
    "    formula_len = torch.tensor([len(f) + 1 for f in formulas], dtype=torch.long)\n",
    "    formulas = pad_sequence(formulas, batch_first=True)\n",
    "\n",
    "    batch_size = len(batch)\n",
    "    sos = torch.full((batch_size, 1), text.sos_id, dtype=torch.long)\n",
    "    eos = torch.full((batch_size, 1), text.eos_id, dtype=torch.long)\n",
    "    formulas = torch.cat((sos, formulas, eos), dim=-1)\n",
    "\n",
    "\n",
    "    images = [i[0] for i in batch]\n",
    "    max_width, max_height = 0, 0\n",
    "    for img in images:\n",
    "        c, h, w = img.size()\n",
    "        max_width = max(max_width, w)\n",
    "        max_height = max(max_height, h)\n",
    "\n",
    "    def pad_image(img):\n",
    "        c, h, w = img.size()\n",
    "        padding = (0, 0, max_width - w, max_height - h)\n",
    "        return torchvision.transforms.functional.pad(img, padding, fill=0)\n",
    "\n",
    "    images = [pad_image(img) for img in images]\n",
    "    images = torch.stack(images).to(dtype=torch.float)\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    return images, formulas, formula_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9de70a",
   "metadata": {},
   "source": [
    "### Enkoder\n",
    "* input je slika s 3 kanala (RGB)\n",
    "* output feature map ima `enc_dim` kanala\n",
    "* `nn.MaxPool2d(2, 1)`  -> asimetricni pooling: praktikuje se za slike teksta, jer je tekst vise sirok nego visok\n",
    "\n",
    "##### Forward pass\n",
    "input tenzor za forward pass: `x: (bs, c, w, h)`\n",
    "\n",
    "`bc = batch size`\n",
    "\n",
    "`c = number of channels`\n",
    "\n",
    "`w = image width`\n",
    "\n",
    "`h = image height`\n",
    "\n",
    "1. enkodovati `x(bc, c_in, w_in, h_in) -> (bc, c_out, w_out, h_out) `\n",
    "    *  `c_out` je `enc_dim`\n",
    "    * `w_out` i `h_out` manji od dimenzija inputa (ofc, conv mreza)\n",
    "2. permutovati -> da bi feature vector bio poslednji\n",
    "3. flattenovati\n",
    "* `encoder_out.shape = (bs, sequence_length = w_out * h_out, d = enc_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "010181d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, encoder_dim: int):\n",
    "        super().__init__()\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1),    # Conv 1\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),           # downsample\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),  # Conv 2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),           # downsample\n",
    "\n",
    "            nn.Conv2d(128, encoder_dim, 3, 1, 1),  # Conv 3\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.encoder_dim = encoder_dim\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        x: (bs, c, w, h)\n",
    "        encoder_out: (bs, seq_len = w*h, feat_dim = d)\n",
    "        \"\"\"\n",
    "        encoder_out = self.feature_encoder(x)        # (bs, c, w, h)\n",
    "        encoder_out = encoder_out.permute(0, 2, 3, 1) # (bs, w, h, c)\n",
    "        bs, w, h, d = encoder_out.size()\n",
    "        encoder_out = encoder_out.view(bs, -1, d)   # flatten spatial dims\n",
    "        return encoder_out\n",
    "        '''class ConvEncoder(nn.Module):\n",
    "    def __init__(self, encoder_dim: int):\n",
    "        super().__init__()\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1),\n",
    "            nn.Conv2d(64, 128, 3, 1),\n",
    "            nn.Conv2d(128, 256, 3, 1),\n",
    "            nn.Conv2d(256, 256, 3, 1),\n",
    "            nn.MaxPool2d(2, 1),\n",
    "            nn.Conv2d(256, 512, 3, 1),\n",
    "            nn.MaxPool2d(1, 2),\n",
    "            nn.Conv2d(512, encoder_dim, 3, 1),\n",
    "        )\n",
    "        self.encoder_dim = encoder_dim\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "            x: (bs, c, w, h)\n",
    "            encoder_out: (batch_size, seq_len = width*height, feat_dim = d)\n",
    "        \"\"\"\n",
    "        encoder_out = self.feature_encoder(x)  # (bs, c, w, h) ali c i w manji\n",
    "        encoder_out = encoder_out.permute(0, 2, 3, 1)  # (bs, w, h, c) poslednja dim je feature depth d\n",
    "        bs, _, _, d = encoder_out.size()\n",
    "        encoder_out = encoder_out.view(bs, -1, d) # izravnaj (w, h) matricu u niz w*h tako da (bs, w, h, c) -> (bs, w*h, c)\n",
    "        return encoder_out'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b4a584-8b42-4d7e-ba92-e030f1d34d6f",
   "metadata": {},
   "source": [
    "# Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "494e9d00-104c-43ae-b330-b05ca5411998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enoder_dim: int = 512, decoder_dim: int = 512, attention_dim: int = 512):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Racunamo kontekst vektor na osnovu sledecih jednacina\n",
    "        e = tanh((Wₕhₜ₋₁ + bₕ) + (WᵥV + bᵥ))\n",
    "        αₜ = Softmax(Wₐ·e + bₐ)\n",
    "        cₜ = ∑ᵢ αₜⁱ vᵢ, where vᵢ ∈ V\n",
    "        \"\"\"\n",
    "        self.decoder_attention = nn.Linear(decoder_dim, attention_dim, bias=False) # W_h * h_{t-1}\n",
    "        self.encoder_attention = nn.Linear(enoder_dim, attention_dim, bias=False) # W_V * V\n",
    "        self.attention = nn.Linear(attention_dim, 1, bias=False)      # W_a * attn\n",
    "\n",
    "        # Softmax će pretvoriti sirove rezultate u raspodelu verovatnoće (težine pažnje).\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, h: Tensor, V: Tensor):\n",
    "        \"\"\"\n",
    "        Izračunaj kontekst vektor tako što pažljivo posmatraš najrelevantnije delove slike.\n",
    "\n",
    "        Argumenti:\n",
    "            h: Prethodno skriveno stanje LSTM dekodera. Oblik: (batch_size, decoder_dim)\n",
    "            V: Mapa karakteristika. Oblik: (batch_size, w * h, encoder_dim)\n",
    "\n",
    "        Povratna vrednost:\n",
    "            context (Tensor): Vektor koji iz mapa karakteristike izvlaci relevantne podatke za generisanje sledeceg karaktera.\n",
    "                            Oblik: (batch_size, decoder_dime)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        attn_1 = self.decoder_attention(h) #(b, decoder_dim) -> (b, attention_dim)\n",
    "        attn_2 = self.encoder_attention(V) #(b, w*h, enoder_dim) -> (b, w*h, attention_dim)\n",
    "\n",
    "        attention= self.attention(torch.tanh(attn_1.unsqueeze(1) + attn_2)).squeeze(2)\n",
    "        # attn_1.unsqueeze(1): (b, 1, attention_dim)\n",
    "        # attn_2: (b, w*h, attention_dim)\n",
    "        # tanh(): (b, w*h, attention_dim)\n",
    "        # attention: (b, w*h, 1) -> squeeze(2) -> (b, w*h)\n",
    "\n",
    "        alpha = self.softmax(attention)\n",
    "\n",
    "\n",
    "        context = (alpha.unsqueeze(2) * V).sum(dim=1)\n",
    "        # alpha.unsqueeze(2): (b, w*h, 1)\n",
    "        # V: (b, w*h, enoder_dim)\n",
    "        # product: (b, w*h, enoder_dim)\n",
    "        # context: (b, enoder_dim)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58861f50-9f77-4a8d-9099-c410a079e676",
   "metadata": {},
   "source": [
    "# Dekoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff92e4ea-d9a2-4c4a-8435-80a4efba5143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,n_class: int,embedding_dim: int = 80,encoder_dim: int = 512,decoder_dim: int = 512,attention_dim: int = 512,\n",
    "        num_layers: int = 1,dropout: float = 0.1,bidirectional: bool = False,sos_id: int = 1,eos_id: int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Implementacija dekodera za Image-to-Latex model.\n",
    "        Koristi LSTM ćeliju i Luong pažnju da generiše LaTeX simbole korak po korak.\n",
    "        cₜ = Attention(hₜ₋₁, V)\n",
    "        eₜ = Embedding(yₜ)\n",
    "        (oₜ, hₜ) = LSTM(hₜ₋₁, [cₜ, eₜ])\n",
    "        p(yₜ₊₁ | y₁, ..., yₜ) = Softmax(Wₒ · oₜ + bₒ)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.sos_id = sos_id\n",
    "        self.eos_id = eos_id\n",
    "\n",
    "        # Embedding layer konvertuje token ID u vektor\n",
    "        self.embedding = nn.Embedding(n_class, embedding_dim)  # (vocab_size, embedding_dim)\n",
    "\n",
    "        # Instanca mehanizma pažnje\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # Veličina enkodera -> veličina pažnje\n",
    "\n",
    "        # Linearni sloj za spajanje embeddinga i konteksta pažnje\n",
    "        self.concat = nn.Linear(embedding_dim + encoder_dim, decoder_dim)  # (embedding_dim + encoder_dim) -> decoder_dim\n",
    "\n",
    "        # Prvi LSTM sloj\n",
    "        self.rnn = nn.LSTM(\n",
    "            decoder_dim,\n",
    "            decoder_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        # Dropout za regularizaciju\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Drugi LSTM sloj za dublji model\n",
    "        self.rnn2 = nn.LSTM(\n",
    "            decoder_dim,\n",
    "            decoder_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        # Izlazni sloj koji preslikuje u prostor rečnika\n",
    "        self.out = nn.Linear(decoder_dim, n_class)  # (decoder_dim) -> (n_class)\n",
    "\n",
    "        # LogSoftmax za stabilnost prilikom računanja gubitka\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        # Inicijalizacija težina\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        if isinstance(layer, nn.Embedding):\n",
    "            nn.init.orthogonal_(layer.weight)\n",
    "        elif isinstance(layer, nn.LSTM):\n",
    "            for name, param in self.rnn.named_parameters():\n",
    "                if name.startswith(\"weight\"):\n",
    "                    nn.init.orthogonal_(param)\n",
    "\n",
    "    def forward(self, y, encoder_out=None, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Generiše sledeći token na osnovu trenutnog stanja i izlaza enkodera.\n",
    "\n",
    "        Argumenti:\n",
    "            y: Ulazni tokeni. Oblik: (batch_size, target_len)\n",
    "            encoder_out: Izlaz enkodera (V). Oblik: (batch_size, encoder_dim, w', h')\n",
    "            hidden_state: Prethodno skriveno stanje (h, c). Oblik: (num_layers * num_directions, batch_size, decoder_dim)\n",
    "\n",
    "        Povratna vrednost:\n",
    "            out: Log-verovatnoće za sledeći token. Oblik: (batch_size, 1, n_class)\n",
    "            hidden_state: Ažurirano skriveno stanje.\n",
    "        \"\"\"\n",
    "\n",
    "        h, c = hidden_state  # (b, decoder_dim), (b, decoder_dim)\n",
    "\n",
    "        embed = self.embedding(y)  # (b, seq_len, embedding_dim)\n",
    "        attention_context = self.attention(h, encoder_out)  # (b, encoder_dim)\n",
    "\n",
    "        rnn_input = torch.cat([embed[:, -1], attention_context], dim=1)  # (b, embedding_dim + encoder_dim)\n",
    "        rnn_input = self.concat(rnn_input)  # (b, decoder_dim)\n",
    "\n",
    "        rnn_input = rnn_input.unsqueeze(1)  # (b, 1, decoder_dim)\n",
    "        hidden_state = (h.unsqueeze(0), c.unsqueeze(0))  # (1, b, decoder_dim), (1, b, decoder_dim)\n",
    "\n",
    "        out, hidden_state = self.rnn(rnn_input, hidden_state)  # out: (b, 1, decoder_dim)\n",
    "\n",
    "        out = self.dropout(out)  # (b, 1, decoder_dim)\n",
    "\n",
    "        out, hidden_state = self.rnn2(out, hidden_state)  # out: (b, 1, decoder_dim)\n",
    "        out = self.logsoftmax(self.out(out))  # (b, 1, n_class)\n",
    "\n",
    "        h, c = hidden_state\n",
    "        return out, (h.squeeze(0), c.squeeze(0))  # Squeeze dimenziju slojeva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da30d7-73bf-4590-8405-46efc392c111",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f4997b2-f22c-44b0-9fff-97bb1c6439d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image2LatexModel(nn.Module):\n",
    "    def __init__(self,n_class: int,embedding_dim: int = 80,encoder_dim: int = 512,decoder_dim: int = 512,attention_dim: int = 512,\n",
    "        num_layers: int = 1,dropout: float = 0.1,bidirectional: bool = False,text: Text = None, beam_width: int = 5, sos_id: int = 1,eos_id: int = 2):\n",
    "        super().__init__()\n",
    "        self.encoder = ConvEncoder(encoder_dim=encoder_dim)\n",
    "        self.decoder = Decoder(n_class=n_class,embedding_dim=embedding_dim,encoder_dim=encoder_dim,decoder_dim=decoder_dim,attention_dim=attention_dim,num_layers=num_layers,dropout=dropout,bidirectional=bidirectional,sos_id=sos_id,eos_id=eos_id)\n",
    "\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.n_class = n_class\n",
    "        self.text = text\n",
    "        self.beam_width = beam_width\n",
    "        self.encoder = ConvEncoder(encoder_dim=encoder_dim)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # TODO: why do we init the hidden state like this and not just 0?\n",
    "    def init_decoder_hidden_state(self, V: Tensor):\n",
    "        \"\"\"\n",
    "            input V je autput enkodera (bs, w*h, c)\n",
    "            return (h, c)\n",
    "        \"\"\"\n",
    "        encoder_mean = V.mean(dim=1)\n",
    "        h = torch.tanh(self.init_h(encoder_mean))\n",
    "        c = torch.tanh(self.init_c(encoder_mean))\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, x: Tensor, y: Tensor, y_len: Tensor):\n",
    "        encoder_out = self.encoder(x)\n",
    "\n",
    "        hidden_state = self.init_decoder_hidden_state(encoder_out)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for t in range(y_len.max().item()):\n",
    "            dec_input = y[:, t].unsqueeze(1)\n",
    "            out, hidden_state = self.decoder(dec_input, encoder_out, hidden_state)\n",
    "            predictions.append(out.squeeze(1))\n",
    "\n",
    "        predictions = torch.stack(predictions, dim=1)\n",
    "        return predictions\n",
    "    \n",
    "    def decode_beam_search(self, x, max_length=150):\n",
    "        encoder_out = self.encoder(x)\n",
    "        hidden_state = self.init_decoder_hidden_state(encoder_out)\n",
    "\n",
    "        list_candidate = [([self.decoder.sos_id], hidden_state, 0)]\n",
    "        for t in range(max_length):\n",
    "            new_candidates = []\n",
    "            for inp, state, log_prob in list_candidate:\n",
    "                y = torch.LongTensor([inp[-1]]).view(BATCH_SIZE, -1).to(x.device)\n",
    "                out, hidden_state = self.decoder(y, encoder_out, state)\n",
    "\n",
    "                topk = out.topk(self.beam_width)\n",
    "                for val, idx in zip(topk.values.view(-1), topk.indices.view(-1)):\n",
    "                    new_inp = inp + [idx.item()]\n",
    "                    new_candidates.append((new_inp, hidden_state, log_prob + val.item()))\n",
    "\n",
    "            new_candidates = sorted(new_candidates, key=lambda x: x[2], reverse=True)\n",
    "            list_candidate = new_candidates[: self.beam_width]\n",
    "\n",
    "        return list_candidate[0][0]\n",
    "    \n",
    "    def decode(self, x, max_length=150):\n",
    "        predict = self.decode_beam_search(x, max_length)\n",
    "        return self.text.int2text(predict)\n",
    "    \n",
    "    def compute_loss(self, outputs, formulas_out):\n",
    "        \"\"\"\n",
    "        Compute CrossEntropy loss between predictions and targets.\n",
    "        outputs: (batch, seq_len, vocab_size) logits\n",
    "        formulas_out: (batch, seq_len) target token IDs\n",
    "        \"\"\"\n",
    "        bs, t, _ = outputs.size()\n",
    "        return self.criterion(\n",
    "            outputs.reshape(bs * t, -1),   # flatten predictions\n",
    "            formulas_out.reshape(-1)       # flatten targets\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e57c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(128),   # Resize the *shorter side* to 128 pixels, keeps aspect ratio\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3de188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def exact_match(pred_list, truth_list):\n",
    "    em_scores = []\n",
    "    for pred, truth in zip(pred_list, truth_list):\n",
    "        len_pred = len(pred)\n",
    "        len_truth = len(truth)\n",
    "        max_len = max(len_pred, len_truth)\n",
    "\n",
    "        # Pad both sequences to the same length\n",
    "        padded_pred = pred + [\"\"] * (max_len - len_pred)\n",
    "        padded_truth = truth + [\"\"] * (max_len - len_truth)\n",
    "\n",
    "        # Calculate EM for this single pair\n",
    "        em = (np.array(padded_pred) == np.array(padded_truth)).all()\n",
    "        em_scores.append(em)\n",
    "\n",
    "    # Return the mean EM score for the entire batch\n",
    "    return torch.tensor(np.mean(em_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f178e-c2d2-49ef-b7ad-8d4defe74d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "MAX_LENGTH=150\n",
    "EFFECTIVE_BATCH_SIZE = 64\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def bind_gpu(data):\n",
    "    device = get_device()\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [bind_gpu(data_elem) for data_elem in data]\n",
    "    else:\n",
    "        return data.to(device, non_blocking=True)\n",
    "\n",
    "# Priprema podataka\n",
    "text_processor = Text()\n",
    "\n",
    "#plz ucitaj ih\n",
    "train_dataset = LatexDataset('/content/data/im2latex_train.csv', transform=transform)\n",
    "val_dataset = LatexDataset('/content/data/im2latex_validate.csv', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # important for training\n",
    "    num_workers=WORKERS,\n",
    "    collate_fn=lambda batch: collate_fn(batch, text)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # usually keep validation deterministic\n",
    "    num_workers=WORKERS,\n",
    "    collate_fn=lambda batch: collate_fn(batch, text)\n",
    ")\n",
    "\n",
    "# Inicijalizacija modela,greske,scheduler,optimizera\n",
    "device = get_device()\n",
    "\n",
    "model = Image2LatexModel(\n",
    "    n_class=text_processor.n_class,\n",
    "    text=text_processor,\n",
    "    beam_width=5,\n",
    "    sos_id=text_processor.sos_id,\n",
    "    eos_id=text_processor.eos_id\n",
    ").to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.002, betas=(0.9, 0.98))\n",
    "\n",
    "total_steps = (len(train_dataset) // EFFECTIVE_BATCH_SIZE) * N_EPOCHS\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.002, total_steps=total_steps)\n",
    "accumulation_steps = EFFECTIVE_BATCH_SIZE // BATCH_SIZE\n",
    "\n",
    "scaler = GradScaler()\n",
    "#treninng\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "\n",
    "        images, formulas, formula_len = bind_gpu(batch)\n",
    "\n",
    "        formulas_in = formulas[:, :-1]\n",
    "        formulas_out = formulas[:, 1:]\n",
    "\n",
    "        with autocast():\n",
    "          outputs = model(images, formulas_in, formula_len)\n",
    "\n",
    "          loss = criterion(outputs.reshape(-1, outputs.shape[-1]), formulas_out.reshape(-1))\n",
    "          loss = loss / accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        train_loss += loss.item() * accumulation_steps\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "        del images, formulas, outputs, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    if (batch_idx + 1) % accumulation_steps != 0:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Average Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # model save\n",
    "    os.makedirs(\"/content/drive/My Drive/models\", exist_ok=True)\n",
    "    path = f\"/content/drive/My Drive/models/model{epoch+1}.pt\"\n",
    "    torch.save(model, path)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_bleu = 0.0\n",
    "    val_em = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "\n",
    "            images, formulas, formula_len = bind_gpu(batch)\n",
    "            formulas_in = formulas[:, :-1]\n",
    "            formulas_out = formulas[:, 1:]\n",
    "\n",
    "            with autocast():\n",
    "              outputs = model(images, formulas_in, formula_len)\n",
    "              loss = criterion(outputs.reshape(-1, outputs.shape[-1]), formulas_out.reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predicts = []\n",
    "            truths = []\n",
    "            for img, formula in zip(images, formulas):\n",
    "                pred_tokens = model.decode(img.unsqueeze(0), max_length=MAX_LENGTH)\n",
    "                truth_tokens = formula.tolist()\n",
    "\n",
    "                predicts.append(pred_tokens)\n",
    "                truths.append(truth_tokens)\n",
    "\n",
    "            predict_strings = [text_processor.tokenize(text_processor.int2text(p)) for p in predicts]\n",
    "            truth_strings = [text_processor.tokenize(text_processor.int2text(t)) for t in truths]\n",
    "            print(predict_strings)\n",
    "            print(truth_strings)\n",
    "            bleu4 = corpus_bleu([[t] for t in truth_strings], predict_strings)\n",
    "            em = exact_match(predict_strings, truth_strings)\n",
    "\n",
    "            val_bleu += bleu4\n",
    "            val_em += em\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_bleu = val_bleu / len(val_loader)\n",
    "    avg_val_em = val_em / len(val_loader)\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, BLEU4: {avg_val_bleu:.4f}, EM: {avg_val_em:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "torch.save(model.state_dict(), 'image2latex_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
